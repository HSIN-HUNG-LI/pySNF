{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c895e111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 6870 rows\n",
      "Filtered dataset size: 56 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "'''\n",
    "Description: This script filters a dataset of SNF (Spent Nuclear Fuel) records\n",
    "based on a list of SNF names provided in a text file. The filtered dataset\n",
    "is saved to a new CSV file.\n",
    "** TSC01 file\n",
    "'''\n",
    "\n",
    "def load_snf_list(txt_path):\n",
    "    \"\"\"\n",
    "    Load SNF IDs from a text file. The text file should contain\n",
    "    comma-separated SNF names (e.g., LJ1099, LJ1109,...).\n",
    "    Returns a list of SNF names.\n",
    "    \"\"\"\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "    # Split by comma and strip whitespace\n",
    "    return [name.strip() for name in content.split(\",\") if name.strip()]\n",
    "\n",
    "\n",
    "def filter_dataset_by_snf(df, snf_list):\n",
    "    \"\"\"\n",
    "    Filter the input DataFrame and return only rows where the 'Name' column\n",
    "    matches one of the SNF names in the list.\n",
    "    \"\"\"\n",
    "    return df[df[\"Name\"].isin(snf_list)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Define file paths\n",
    "    csv_path = \"data/all_stdh_dataset.csv\"\n",
    "    snf_txt_path = \"data/TSC01_SNFs.txt\"\n",
    "\n",
    "    # Load dataset and SNF list\n",
    "    df = pd.read_csv(csv_path)\n",
    "    snf_list = load_snf_list(snf_txt_path)\n",
    "\n",
    "    # Filter dataset\n",
    "    filtered_df = filter_dataset_by_snf(df, snf_list)\n",
    "\n",
    "    # Optional: print result summary\n",
    "    print(f\"Original dataset size: {len(df)} rows\")\n",
    "    print(f\"Filtered dataset size: {len(filtered_df)} rows\")\n",
    "\n",
    "    # Save filtered result if needed\n",
    "    filtered_df.to_csv(\"all_stdh_dataset_tsc01.csv\", index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531882ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1A0016', '1A0026', '1A0009', '1A0003', '1A0066', '1A0098', '1A0319', '1A0103', '1A0198', '1A0329', '1A0407', '1A0028', '1A0272', '1A0119', '1B0548', '1B0506', '1B0513', '1A0238', '1A0248', '1A0007', '1A0406', '1A0089', '1B0541', '1B0501', '1B0547', '1A0311', '1A0273', '1A0012', '1A0013', '1A0338', '1A0088', '1B0430', '1B0475', '1B0516', '1A0306', '1A0190', '1A0015', '1A0014', '1A0072', '1B0500', '1B0514', '1B0542', '1A0297', '1A0361', '1A0360', '1A0138', '1A0149', '1A0373', '1A0385', '1A0218', '1A0129', '1A0010', '1A0068', '1A0109', '1A0011', '1A0018']\n"
     ]
    }
   ],
   "source": [
    "#===============================================================\n",
    "### Convert different Type -> Code to map 'Type' values in a dataset\n",
    "#===============================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/all_stdh_dataset.csv')\n",
    "\n",
    "with open('TSC01_SNFs.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "names = [n.strip() for n in text.replace(',', '\\n').splitlines() if n.strip()]\n",
    "\n",
    "\n",
    "mapping = df.set_index('Name')['SNF_id']\n",
    "snf_ids = mapping.loc[names].tolist()\n",
    "\n",
    "print(snf_ids)\n",
    "with open('TSC01_SNF_id.txt', 'w') as f:\n",
    "    f.write(', '.join(snf_ids))\n",
    "pd.DataFrame({'SNF_id': snf_ids}).to_csv('TSC01_SNF_id.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29bb58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'Type' values:\n",
      "['GE88-1' 'GE88-2'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#===============================================================\n",
    "### Convert different Type -> Code to map 'Type' values in a dataset\n",
    "#===============================================================\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "project_root = Path.cwd().resolve().parents[1]\n",
    "data_file = project_root / \"data\" / \"test_files\" / \"all_stdh_dataset_tsc01.csv\"\n",
    "df = pd.read_csv(data_file)\n",
    "type_map = {\n",
    "    \"GE88-1\": \"TypeA\",\n",
    "    \"GE88-2\": \"TypeB\",\n",
    "    \"Atrium10\": \"TypeC\",\n",
    "    \"SPC88\": \"TypeD\",\n",
    "    \"GE9B\": \"TypeE\"\n",
    "}\n",
    "\n",
    "print(\"Original 'Type' values:\")\n",
    "print(df[\"Type\"].unique(), \"\\n\")\n",
    "\n",
    "unknown = set(df[\"Type\"].unique()) - set(type_map.keys())\n",
    "if unknown:\n",
    "    print(f\"❗ Found unmapped Type values: {unknown}\\n\")\n",
    "    print(\"Rows with unmapped Type values:\")\n",
    "    print(df[df[\"Type\"].isin(unknown)])\n",
    "    raise ValueError(f\"Unmapped Type values found: {unknown}\")\n",
    "\n",
    "df[\"Type\"] = df[\"Type\"].replace(type_map)\n",
    "\n",
    "df.to_csv(data_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a574b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================\n",
    "###  Create Test file for each page \n",
    "#===============================================================\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define input and output paths\n",
    "project_root = Path.cwd().resolve().parents[2] \n",
    "input_path = Path(project_root/\"pySNF/data/DataBase_SNFs/all_stdh_dataset.csv\")\n",
    "# input_path = Path(project_root/\"pySNF/data/test_files/all_stdh_dataset.csv\")\n",
    "output_path = Path(project_root/\"pySNF/data/TEST_all_snfs/All_SNFs_Id.csv\")\n",
    "\n",
    "# Load the original dataset\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Keep only the specified columns\n",
    "# selected_columns = ['Enrich', 'SP', 'Burnup', 'Cool']\n",
    "selected_id = ['SNF_id']\n",
    "# df_filtered = df[selected_columns]\n",
    "df_filtered = df[selected_id]\n",
    "\n",
    "# Compute the mean for each of the four columns\n",
    "# mean_values = df[columns_to_average].mean()\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV\n",
    "df_filtered.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Filtered dataset saved to: {output_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f63331",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Rename all SNF detail CSV files in DataBase_SNFs/ according to the SNF_id mapping \n",
    "provided in all_stdh_dataset.csv, then delete the original files.\n",
    "\n",
    "Example:\n",
    "    LJ1084_CipMTU.csv   --> 1A0001_CipMTU.csv (original LJ1084_CipMTU.csv removed)\n",
    "    LJ1084_gpMTU.csv    --> 1A0001_gpMTU.csv (original LJ1084_gpMTU.csv removed)\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def main():\n",
    "    # 1. Set up paths\n",
    "    base_dir = Path(\"DataBase_SNFs\")\n",
    "    index_path = base_dir / \"all_stdh_dataset.csv\"\n",
    "\n",
    "    # 2. Load the index dataset\n",
    "    try:\n",
    "        df_index = pd.read_csv(index_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Could not read index file '{index_path}': {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 3. Ensure required columns exist\n",
    "    required_cols = {\"Name\", \"SNF_id\"}\n",
    "    if not required_cols.issubset(df_index.columns):\n",
    "        print(f\"Error: Index file must contain columns: {required_cols}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 4. Build mapping from Name to SNF_id (strings)\n",
    "    df_index[\"Name\"] = df_index[\"Name\"].astype(str)\n",
    "    df_index[\"SNF_id\"] = df_index[\"SNF_id\"].astype(str)\n",
    "    name_to_id = dict(zip(df_index[\"Name\"], df_index[\"SNF_id\"]))\n",
    "\n",
    "    # 5. Process each CSV in the folder, skipping the index file\n",
    "    for file_path in base_dir.glob(\"*.csv\"):\n",
    "        if file_path.name == index_path.name:\n",
    "            continue  # skip the master index\n",
    "\n",
    "        stem = file_path.stem  # e.g. \"LJ1084_CipMTU\"\n",
    "        parts = stem.split(\"_\", 1)\n",
    "\n",
    "        # 6. Validate filename format\n",
    "        if len(parts) != 2:\n",
    "            print(f\"Warning: Skipping file with unexpected name format: {file_path.name}\")\n",
    "            continue\n",
    "\n",
    "        name_prefix, suffix = parts\n",
    "\n",
    "        # 7. Lookup SNF_id\n",
    "        if name_prefix not in name_to_id:\n",
    "            print(f\"Error: No matching 'Name' entry for file '{file_path.name}'\")\n",
    "            continue\n",
    "\n",
    "        new_id = name_to_id[name_prefix]\n",
    "        new_filename = f\"{new_id}_{suffix}.csv\"\n",
    "        new_path = base_dir / new_filename\n",
    "\n",
    "        # 8. Read, write under new name, then delete original\n",
    "        try:\n",
    "            df_temp = pd.read_csv(file_path)\n",
    "            df_temp.to_csv(new_path, index=False)\n",
    "            file_path.unlink()\n",
    "            print(f\"Renamed and deleted '{file_path.name}' → '{new_filename}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing '{file_path.name}': {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39a3e3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: 'Name' column removed and 'all_stdh_dataset.csv' updated.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Load 'all_stdh_dataset.csv', drop the 'Name' column if present,\n",
    "and overwrite the original file with the updated DataFrame.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def remove_name_column(csv_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Reads the CSV at csv_path, removes the 'Name' column, and saves\n",
    "    the result back to the same path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Cannot read '{csv_path}': {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    if \"Name\" not in df.columns:\n",
    "        print(f\"Warning: 'Name' column not found in '{csv_path}'. No changes made.\")\n",
    "        return\n",
    "\n",
    "    # Drop the 'Name' column\n",
    "    df = df.drop(columns=[\"Name\"])\n",
    "\n",
    "    try:\n",
    "        # Overwrite the original CSV\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Success: 'Name' column removed and '{csv_path.name}' updated.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Cannot write to '{csv_path}': {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "def main():\n",
    "    # Path to the master dataset CSV\n",
    "    project_root = Path.cwd().resolve().parents[2] \n",
    "    csv_file = Path(project_root/\"pySNF/data/DataBase_SNFs/all_stdh_dataset.csv\")\n",
    "\n",
    "    if not csv_file.exists():\n",
    "        print(f\"Error: File not found: {csv_file}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    remove_name_column(csv_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b51e4eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Columns reordered and 'Prediction_all_batch.csv' updated.\n",
      "Success: Added 'S/n' column and updated 'Prediction_all_batch.csv'.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Read 'DataBase_SNFs/Prediction_all_batch.csv', insert a serial-number column 'S/n'\n",
    "as the first column (1, 2, 3, …), and overwrite the original CSV file.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def add_serial_column(csv_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Load the CSV at csv_path, insert a 1-based 'S/n' column at the front,\n",
    "    and save back to the same path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Failed to read '{csv_path}': {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Insert serial numbers 1,2,... as the first column named \"S/n\"\n",
    "    df.insert(0, \"S/n\", range(1, len(df) + 1))\n",
    "\n",
    "    try:\n",
    "        # Overwrite the original CSV without the index\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Success: Added 'S/n' column and updated '{csv_path.name}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Failed to write '{csv_path}': {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "def reorder_columns(csv_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Load the CSV at csv_path, reorder the columns to the specified sequence,\n",
    "    and save back to the same path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Failed to read '{csv_path}': {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Desired column order\n",
    "    new_order = [\"Burnup\", \"Cool\", \"Enrich\", \"SP\"]\n",
    "    # Validate that all required columns are present\n",
    "    missing = [col for col in new_order if col not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"Error: The following required columns are missing: {missing}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Reorder DataFrame; any extra columns will be dropped\n",
    "    df_reordered = df[new_order]\n",
    "\n",
    "    try:\n",
    "        # Overwrite the original CSV without including the pandas index\n",
    "        df_reordered.to_csv(csv_path, index=False)\n",
    "        print(f\"Success: Columns reordered and '{csv_path.name}' updated.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Failed to write '{csv_path}': {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "def main():\n",
    "    # Define path to the batch predictions CSV\n",
    "    project_root = Path.cwd().resolve().parents[2] \n",
    "    input_path = Path(project_root/\"pySNF/data/TEST_prediction\")\n",
    "    csv_file = input_path / \"Prediction_all_batch.csv\"\n",
    "    if not csv_file.exists():\n",
    "        print(f\"Error: File not found: {csv_file}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # add_serial_column(csv_file)\n",
    "    reorder_columns(csv_file)\n",
    "    add_serial_column(csv_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tkinter_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
