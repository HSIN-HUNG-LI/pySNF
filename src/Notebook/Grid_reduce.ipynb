{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4c5f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Script Description:\n",
    "# This script loads the original SNF interpolation grid database (grid_database.parq),\n",
    "# reduces the grid points according to a new experiment setting (exp_folder_name),\n",
    "# and saves the reduced dataset as a new Parquet file.\n",
    "#\n",
    "# Example:\n",
    "#   exp_folder_name = \"1412\"\n",
    "#   => enrich_factor = 1, sp_factor = 4, bp_factor = 1, cool_factor = 2\n",
    "#\n",
    "# The reduced dataset is written to: grid_database_1412.parq\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def reduce_grid_database(input_file: Path, exp_folder_name: str, output_file: Path) -> None:\n",
    "    \"\"\"\n",
    "    Reduce the interpolation grid size according to the given experiment factors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_file : Path\n",
    "        Path to the original grid_database.parq file.\n",
    "    exp_folder_name : str\n",
    "        Four-digit string encoding reduction factors for [enrich, sp, burnup, cool].\n",
    "    output_file : Path\n",
    "        Path to save the reduced grid database in Parquet format.\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 1: Parse reduction factors from exp_folder_name\n",
    "    # -------------------------------------------------------------------------\n",
    "    enrich_factor = int(exp_folder_name[0])\n",
    "    sp_factor = int(exp_folder_name[1])\n",
    "    bp_factor = int(exp_folder_name[2])\n",
    "    cool_factor = int(exp_folder_name[3])\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 2: Define the original full interpolation spaces\n",
    "    # -------------------------------------------------------------------------\n",
    "    enrich_space = np.arange(1.5, 6.1, 0.5)[0::enrich_factor]\n",
    "    sp_space = np.arange(5, 46, 5)[0::sp_factor]\n",
    "    burnup_space = np.arange(5000, 74100, 3000)[0::bp_factor]\n",
    "    cool_space_raw = np.logspace(-5.75, 6.215, 150, base=math.e)\n",
    "    cool_space = cool_space_raw[1::cool_factor]\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 3: Load the original dataset\n",
    "    # -------------------------------------------------------------------------\n",
    "    df = pd.read_parquet(input_file)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 4: Filter the dataset according to reduced interpolation spaces\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_reduced = df[\n",
    "        df[\"Enrich\"].isin(enrich_space)\n",
    "        & df[\"SP\"].isin(sp_space)\n",
    "        & df[\"Burnup\"].isin(burnup_space)\n",
    "        & df[\"Cool\"].isin(cool_space)\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 5: Save the reduced dataset\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_reduced.to_parquet(output_file, index=False)\n",
    "    print(\n",
    "        f\"Reduced grid database saved to {output_file} \"\n",
    "        f\"with {df_reduced.shape[0]} rows (from {df.shape[0]} original rows).\"\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths\n",
    "    project_root = Path.cwd()\n",
    "    input_data_file = project_root / \"data\" / \"grid_database.parq\"\n",
    "    exp_folder_name = \"1412\"\n",
    "    output_data_file = project_root / \"data\" / f\"grid_database_{exp_folder_name}.parq\"\n",
    "\n",
    "    # Run the reduction process\n",
    "    reduce_grid_database(input_data_file, exp_folder_name, output_data_file)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
